{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of python libraries\n",
    "\n",
    "In this section there are all the libraries used in the entire file listed so that it may be easy to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from math import sqrt\n",
    "from geopy.geocoders import Nominatim\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used along the code\n",
    "\n",
    "All the function used along the code are listed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_population(city):\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_largest_cities_and_towns_in_Turkey'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='wikitable')\n",
    "    rows = table.find_all('tr')[1:]  # Exclude the header row\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if columns[1].text.strip() == city:\n",
    "            population = columns[6].text.strip()\n",
    "            return population\n",
    "    return 'Population not found'\n",
    "\n",
    "def latitude(city):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.geocode(city)\n",
    "    if location is not None:\n",
    "        latitude = location.latitude\n",
    "        return latitude\n",
    "    else:\n",
    "        return \"Latitude not found\"\n",
    "    \n",
    "def longitude(city):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.geocode(city)\n",
    "    if location is not None:\n",
    "        longitude = location.longitude\n",
    "        return longitude\n",
    "    else:\n",
    "        return \"Longitude not found\"\n",
    "    \n",
    "def season(date):\n",
    "    month = date.strftime('%B')\n",
    "    day = date.day\n",
    "    if month in ('January', 'February', 'March'):\n",
    "        season = 'winter'\n",
    "    elif month in ('April', 'May', 'June'):\n",
    "        season = 'spring'\n",
    "    elif month in ('July', 'August', 'September'):\n",
    "        season = 'summer'\n",
    "    else:\n",
    "        season = 'autumn'\n",
    "    if (month == 'March') and (day > 20):\n",
    "        season = 'spring'\n",
    "    elif (month == 'June') and (day > 20):\n",
    "        season = 'summer'\n",
    "    elif (month == 'September') and (day > 22):\n",
    "        season = 'autumn'\n",
    "    elif (month == 'December') and (day > 21):\n",
    "        season = 'winter'\n",
    "    return season\n",
    "\n",
    "def holidays(date):\n",
    "    year = date.strftime('%Y')\n",
    "    formated = date.strftime('%#d %B')\n",
    "    url = 'https://en.wikipedia.org/wiki/Public_holidays_in_Turkey'\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the holiday data\n",
    "    table = soup.find_all('table', 'wikitable')[0]\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        # Extract the cells of each row\n",
    "        columns = row.find_all('td')\n",
    "        if columns[0].text.strip() == formated:\n",
    "            name = columns[1].text.strip()\n",
    "            return name\n",
    "    return 'Not Holiday'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "In this section some data cleaning will be performed. The city names were corrected, null values were replaced using the interpolate function of pandas, aswell as a k-nearest neighbors algorithm, for the first 2 csv files.  \n",
    "For the third csv file, some columns were droped, and null values were filed, around 7000 null values remain which means that ir represents less than 1 % of the entire csv file, so we made the decision to drop those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.read_csv(\"cities.csv\")\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('?', 'i')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Sanliurfa', 'Şanlıurfa')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Izmir', 'İzmir')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Diyarbakir', 'Diyarbakır')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Eskiiehir', 'Eskişehir')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Adapazari', 'Adapazarı')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Kahramanmaras', 'Kahramanmaraş')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv(\"product.csv\")\n",
    "\n",
    "# Check for nulls and replace missing values\n",
    "print(product_df.isna().sum())\n",
    "product_df[\"product_length\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "product_df[\"product_depth\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "product_df[\"product_width\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "print(product_df.isna().sum())\n",
    "\n",
    "# Split the data into complete and missing\n",
    "complete_data = product_df.dropna()\n",
    "missing_data = product_df[product_df['cluster_id'].isnull()].drop('cluster_id', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    complete_data[['product_length', 'product_depth', 'product_width']],\n",
    "    complete_data['cluster_id'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform K-nearest neighbors classification with hyperparameter tuning\n",
    "parameters = {\n",
    "    'n_neighbors': range(1, 21),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_weights = grid_search.best_params_['weights']\n",
    "best_algorithm = grid_search.best_params_['algorithm']\n",
    "best_leaf_size = grid_search.best_params_['leaf_size']\n",
    "best_p = grid_search.best_params_['p']\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Predict clusters for missing data using the best model\n",
    "missing_data['cluster_id'] = best_model.predict(missing_data[['product_length', 'product_depth', 'product_width']])\n",
    "\n",
    "# Concatenate the complete data with the imputed missing data\n",
    "result_df = pd.concat([complete_data, missing_data])\n",
    "\n",
    "# Print the resulting DataFrame with clusters\n",
    "print(result_df)\n",
    "print(result_df.isna().sum())\n",
    "\n",
    "result_df.to_csv(\"product_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(\"sales.csv\")\n",
    "sales_df = sales_df[sales_df[\"date\"] < \"2019-10-01\"]\n",
    "sales_df = sales_df.drop(sales_df.columns[0], axis=1)\n",
    "print(sales_df.isna().sum())\n",
    "\n",
    "#fill the price\n",
    "sales_df['price'] = sales_df.groupby('product_id')['price'].transform(lambda x: x.fillna(x.mean()))\n",
    "print(sales_df.isna().sum())\n",
    "#after this some nulls still remain, that means that the dataset contains products for which there isnt a price in any place\n",
    "\n",
    "#drop columns with a lot of nulls\n",
    "sales_df.drop(['promo_bin_2', 'promo_discount_2', 'promo_discount_type_2'], axis=1, inplace=True)\n",
    "\n",
    "#count distinct values of a column\n",
    "value_counts = sales_df['promo_type_2'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "# Given that less than one percent of the data related to promo_type_2 is different from the majority of the values\n",
    "# we will drop this column aswell\n",
    "sales_df.drop(['promo_type_2'], axis=1, inplace=True)\n",
    "\n",
    "value_counts = sales_df['promo_type_1'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "value_counts = sales_df['promo_bin_1'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "sales_df['promo_bin_1'].fillna(\"none\", inplace=True)\n",
    "\n",
    "#binary_cols = pd.get_dummies(sales_df['promo_bin_1'], prefix='promo_bin_')\n",
    "#sales_df = pd.concat([sales_df, binary_cols], axis=1)\n",
    "\n",
    "#binary_cols = pd.get_dummies(sales_df['promo_type_1'], prefix='promo_type_')\n",
    "#sales_df = pd.concat([sales_df, binary_cols], axis=1)\n",
    "\n",
    "#sales_df.drop(['promo_type_1', 'promo_bin_1'], axis=1, inplace=True)\n",
    "\n",
    "# Dropping the remaing null values of the price its around 7000 so less then 1% see after model if it affects a lot\n",
    "sales_df.dropna(inplace=True)\n",
    "print(sales_df.isna().sum())\n",
    "sales_df.head()\n",
    "sales_df.to_csv(\"sales_df.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding external variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to store population\n",
    "cities_df['Population'] = cities_df['city_code'].apply(get_city_population)\n",
    "cities_df['Latitude'] = cities_df['city_code'].apply(latitude)\n",
    "cities_df['Longitude'] = cities_df['city_code'].apply(longitude)\n",
    "\n",
    "print(cities_df)\n",
    "\n",
    "#It doesnt work for Izmir so replace manually\n",
    "cities_df['Population'] = cities_df['Population'].str.replace('Population not found', '2,847,691')\n",
    "\n",
    "# remove the commas\n",
    "cities_df['Population'] = cities_df['Population'].str.replace(',', '')\n",
    "\n",
    "cities_df.to_csv(\"cities_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"sales_df.csv\")\n",
    "df_1 = df_1[['date']].copy()\n",
    "df_1.drop_duplicates(inplace=True)\n",
    "df_1['date'] = pd.to_datetime(df_1['date'])\n",
    "print(df_1)\n",
    "df_1['season'] = df_1['date'].apply(season)\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['holidays'] = df_1['date'].apply(holidays)\n",
    "print(df_1)\n",
    "\n",
    "#change some values manually\n",
    "df_1.to_csv(\"season_holidays.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sales_df.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "aggregated_df = df.groupby(['date', 'store_id']).agg({'sales': 'sum'}).reset_index()\n",
    "\n",
    "grouped_df = aggregated_df.groupby('store_id')\n",
    "for store_id, group_data in grouped_df:\n",
    "    # Perform time series analysis on the group_data DataFrame\n",
    "    # You can use methods like rolling mean, exponential smoothing, etc.\n",
    "    # Example:\n",
    "    rolling_mean = group_data['sales'].rolling(window=7).mean()\n",
    "    \n",
    "    # Plot the time series\n",
    "    plt.figure()\n",
    "    plt.plot(group_data['date'], group_data['sales'], label='Sales')\n",
    "    plt.plot(group_data['date'], rolling_mean, label='Rolling Mean')\n",
    "    plt.title(f\"Store ID: {store_id}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming data for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unique_combinations = sales_df[['store_id', 'date']].drop_duplicates()\n",
    "merged_df = unique_combinations.merge(cities_df, on='store_id', how='left')\n",
    "print(merged_df)\n",
    "unique_combinations_2 = merged_df[['city_code', 'date']].drop_duplicates()\n",
    "print(unique_combinations_2)\n",
    "print(len(unique_combinations_2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])  # Convert 'date' column to datetime type\n",
    "\n",
    "future_ts = sales_df.groupby([pd.Grouper(key='date', freq='W-MON'), 'store_id']) \\\n",
    "    .agg(total_sales=('sales', 'sum'),\n",
    "         total_revenue=('revenue', 'sum'),\n",
    "         total_stock=('stock', 'sum'),\n",
    "         average_price=('price', 'mean'),\n",
    "         promo_type_count=('promo_type_1', 'nunique'),\n",
    "         promo_bin_count=('promo_bin_1', 'nunique')) \\\n",
    "    .round()\n",
    "\n",
    "print(future_ts)\n",
    "future_ts.head\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING AREA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "sales_df = pd.read_csv(\"sales_df.csv\")\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "one_store = sales_df.query(\"store_id == 'S0002'\")\n",
    "one_store.drop(['store_id'], axis=1, inplace=True)\n",
    "future_ts = one_store.groupby([pd.Grouper(key='date', freq='W-MON')]) \\\n",
    "    .agg(total_sales=('sales', 'sum'),\n",
    "         total_revenue=('revenue', 'sum'),\n",
    "         average_price=('price', 'mean'),\n",
    "         promo_type_count=('promo_type_1', 'nunique'),\n",
    "         promo_bin_count=('promo_bin_1', 'nunique')) \\\n",
    "    .round()\n",
    "\n",
    "train_data = future_ts[:-2]  \n",
    "test_data = future_ts[-2:] \n",
    "\n",
    "X_train = train_data[['total_revenue', 'average_price', 'promo_type_count','promo_bin_count']]  \n",
    "y_train = train_data['total_sales'] \n",
    "\n",
    "X_test = train_data[['total_revenue', 'average_price', 'promo_type_count','promo_bin_count']]  \n",
    "y_test = train_data['total_sales']  \n",
    "\n",
    "\n",
    "#Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação do modelo\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Tunning do modelo\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Criar o modelo de ensemble (Random Forest)\n",
    "model_bp = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Realizar busca em grade para encontrar os melhores parâmetros\n",
    "grid_search = GridSearchCV(estimator=model_bp, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print('Melhores parâmetros:', best_params)\n",
    "\n",
    "# Avaliação do modelo com os melhores parâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Arima weekly (number of sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    store_id  date  week_count\n",
      "0      S0002  2017          52\n",
      "1      S0002  2018          53\n",
      "2      S0002  2019          39\n",
      "3      S0003  2017          52\n",
      "4      S0003  2018          53\n",
      "..       ...   ...         ...\n",
      "174    S0142  2018          53\n",
      "175    S0142  2019          39\n",
      "176    S0143  2017          52\n",
      "177    S0143  2018          53\n",
      "178    S0143  2019          39\n",
      "\n",
      "[179 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sales_df.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "grouped_df = df.groupby([pd.Grouper(key='date', freq='W-MON'), 'store_id']).agg({'sales': 'sum'}).reset_index()\n",
    "\n",
    "# Count the number of weeks in each year for each store\n",
    "week_counts = grouped_df.groupby(['store_id', grouped_df['date'].dt.year]).size().reset_index(name='week_count')\n",
    "print(week_counts)\n",
    "week_counts.to_csv(\"week_counts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "df = pd.read_csv(\"sales_df.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Step 2: Group the data by store and week\n",
    "grouped_df = df.groupby([pd.Grouper(key='date', freq='W-MON'), 'store_id']).agg({'sales': 'sum'}).reset_index()\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "test_start_week = grouped_df['date'].max() - pd.DateOffset(weeks=4)\n",
    "print(test_start_week)\n",
    "train_data = grouped_df[grouped_df['date'] <= test_start_week]\n",
    "print(train_data)\n",
    "test_data = grouped_df[grouped_df['date'] > test_start_week]\n",
    "print(test_data)\n",
    "\n",
    "# Step 4: Perform autoarima on each store's time series\n",
    "predictions = pd.DataFrame()\n",
    "for store, store_data in train_data.groupby('store_id'):\n",
    "    print(store)\n",
    "    print(store_data)\n",
    "    model = auto_arima(store_data['sales'], )\n",
    "    model.fit(store_data['sales'])\n",
    "    forecast_start_week = train_data[train_data['store_id'] == store]['date'].max() + pd.DateOffset(weeks=1)\n",
    "    print(forecast_start_week)\n",
    "    future_weeks = len(test_data[test_data['store_id'] == store])\n",
    "    print(future_weeks)\n",
    "    future_forecast = model.predict(n_periods=future_weeks, start=forecast_start_week)\n",
    "    print(future_forecast)\n",
    "    predictions = predictions.append(pd.DataFrame(future_forecast, columns=[store]).set_index(test_data[test_data['store_id'] == store].index))\n",
    "    index=test_data[test_data['store_id'] == store].index,\n",
    "    print(predictions)\n",
    "\n",
    "    # Step 5: Evaluate the model performance\n",
    "    actual_sales = test_data.loc[test_data['store_id'] == store, 'sales']\n",
    "    print(actual_sales)\n",
    "    predicted_sales = predictions.loc[test_data['store_id'] == store, store]\n",
    "    print(predicted_sales)\n",
    "\n",
    "\n",
    "    mae = mean_absolute_error(actual_sales, predicted_sales)\n",
    "    rmse = mean_squared_error(actual_sales, predicted_sales, squared=False)\n",
    "    mape = (abs((actual_sales - predicted_sales) / actual_sales)).mean() * 100\n",
    "\n",
    "    print(f\"Metrics for Store {store}:\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAPE: {mape}%\")\n",
    "\n",
    "# Step 6: Visualize the results\n",
    "for store in predictions.columns:\n",
    "    plt.figure()\n",
    "    plt.plot(train_data[train_data['store_id'] == store]['sales'], label='Train')\n",
    "    plt.plot(test_data[test_data['store_id'] == store]['sales'], label='Test')\n",
    "    plt.plot(predictions.loc[test_data['store_id'] == store, store], label='Predicted')\n",
    "    plt.title(f\"Sales Prediction - Store {store}\")\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Arima Weekly (revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "df = pd.read_csv(\"sales_df.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Step 2: Group the data by store and week\n",
    "grouped_df = df.groupby([pd.Grouper(key='date', freq='W-MON'), 'store_id']).agg({'revenue': 'sum'}).reset_index()\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "test_start_week = grouped_df['date'].max() - pd.DateOffset(weeks=4)\n",
    "print(test_start_week)\n",
    "train_data = grouped_df[grouped_df['date'] <= test_start_week]\n",
    "print(train_data)\n",
    "test_data = grouped_df[grouped_df['date'] > test_start_week]\n",
    "print(test_data)\n",
    "\n",
    "# Step 4: Perform autoarima on each store's time series\n",
    "predictions = pd.DataFrame()\n",
    "for store, store_data in train_data.groupby('store_id'):\n",
    "    print(store)\n",
    "    print(store_data)\n",
    "    model = auto_arima(store_data['revenue'])\n",
    "    model.fit(store_data['revenue'])\n",
    "    forecast_start_week = train_data[train_data['store_id'] == store]['date'].max() + pd.DateOffset(weeks=1)\n",
    "    print(forecast_start_week)\n",
    "    future_weeks = len(test_data[test_data['store_id'] == store])\n",
    "    print(future_weeks)\n",
    "    future_forecast = model.predict(n_periods=future_weeks, start=forecast_start_week)\n",
    "    print(future_forecast)\n",
    "    predictions = predictions.append(pd.DataFrame(future_forecast, columns=[store]).set_index(test_data[test_data['store_id'] == store].index))\n",
    "    index=test_data[test_data['store_id'] == store].index,\n",
    "    print(predictions)\n",
    "\n",
    "    # Step 5: Evaluate the model performance\n",
    "    actual_revenue = test_data.loc[test_data['store_id'] == store, 'revenue']\n",
    "    print(actual_revenue)\n",
    "    predicted_revenue = predictions.loc[test_data['store_id'] == store, store]\n",
    "    print(predicted_revenue)\n",
    "\n",
    "    mae = mean_absolute_error(actual_revenue, predicted_revenue)\n",
    "    rmse = mean_squared_error(actual_revenue, predicted_revenue, squared=False)\n",
    "    mape = (abs((actual_revenue - predicted_revenue) / actual_revenue)).mean() * 100\n",
    "\n",
    "    print(f\"Metrics for Store {store}:\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAPE: {mape}%\")\n",
    "\n",
    "# Step 6: Visualize the results\n",
    "for store in predictions.columns:\n",
    "    plt.figure()\n",
    "    plt.plot(train_data[train_data['store_id'] == store]['revenue'], label='Train')\n",
    "    plt.plot(test_data[test_data['store_id'] == store]['revenue'], label='Test')\n",
    "    plt.plot(predictions.loc[test_data['store_id'] == store, store], label='Predicted')\n",
    "    plt.title(f\"Revenue Prediction - Store {store}\")\n",
    "    plt.xlabel(\"Week\")\n",
    "    plt.ylabel(\"Revenue\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
