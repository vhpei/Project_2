{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of python libraries\n",
    "\n",
    "In this section there are all the libraries used in the entire file listed so that it may be easy to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from math import sqrt\n",
    "from geopy.geocoders import Nominatim\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used along the code\n",
    "\n",
    "All the function used along the code are listed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_population(city):\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_largest_cities_and_towns_in_Turkey'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='wikitable')\n",
    "    rows = table.find_all('tr')[1:]  # Exclude the header row\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if columns[1].text.strip() == city:\n",
    "            population = columns[6].text.strip()\n",
    "            return population\n",
    "    return 'Population not found'\n",
    "\n",
    "def latitude(city):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.geocode(city)\n",
    "    if location is not None:\n",
    "        latitude = location.latitude\n",
    "        return latitude\n",
    "    else:\n",
    "        return \"Latitude not found\"\n",
    "    \n",
    "def longitude(city):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.geocode(city)\n",
    "    if location is not None:\n",
    "        longitude = location.longitude\n",
    "        return longitude\n",
    "    else:\n",
    "        return \"Longitude not found\"\n",
    "    \n",
    "def season(date):\n",
    "    month = date.strftime('%B')\n",
    "    day = date.day\n",
    "    if month in ('January', 'February', 'March'):\n",
    "        season = 'winter'\n",
    "    elif month in ('April', 'May', 'June'):\n",
    "        season = 'spring'\n",
    "    elif month in ('July', 'August', 'September'):\n",
    "        season = 'summer'\n",
    "    else:\n",
    "        season = 'autumn'\n",
    "    if (month == 'March') and (day > 20):\n",
    "        season = 'spring'\n",
    "    elif (month == 'June') and (day > 20):\n",
    "        season = 'summer'\n",
    "    elif (month == 'September') and (day > 22):\n",
    "        season = 'autumn'\n",
    "    elif (month == 'December') and (day > 21):\n",
    "        season = 'winter'\n",
    "    return season\n",
    "\n",
    "def holidays(date):\n",
    "    year = date.strftime('%Y')\n",
    "    formated = date.strftime('%#d %B')\n",
    "    url = 'https://en.wikipedia.org/wiki/Public_holidays_in_Turkey'\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the holiday data\n",
    "    table = soup.find_all('table', 'wikitable')[0]\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        # Extract the cells of each row\n",
    "        columns = row.find_all('td')\n",
    "        if columns[0].text.strip() == formated:\n",
    "            name = columns[1].text.strip()\n",
    "            return name\n",
    "    return 'Not Holiday'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "In this section some data cleaning will be performed. The city names were corrected, null values were replaced using the interpolate function of pandas, aswell as a k-nearest neighbors algorithm, for the first 2 csv files.  \n",
    "For the third csv file, some columns were droped, and null values were filed, around 7000 null values remain which means that ir represents less than 1 % of the entire csv file, so we made the decision to drop those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.read_csv(\"cities.csv\")\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('?', 'i')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Sanliurfa', 'Şanlıurfa')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Izmir', 'İzmir')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Diyarbakir', 'Diyarbakır')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Eskiiehir', 'Eskişehir')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Adapazari', 'Adapazarı')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Kahramanmaras', 'Kahramanmaraş')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id         0\n",
      "product_length    18\n",
      "product_depth     16\n",
      "product_width     16\n",
      "cluster_id        50\n",
      "hierarchy1_id      0\n",
      "hierarchy2_id      0\n",
      "hierarchy3_id      0\n",
      "hierarchy4_id      0\n",
      "hierarchy5_id      0\n",
      "dtype: int64\n",
      "product_id         0\n",
      "product_length     0\n",
      "product_depth      0\n",
      "product_width      0\n",
      "cluster_id        50\n",
      "hierarchy1_id      0\n",
      "hierarchy2_id      0\n",
      "hierarchy3_id      0\n",
      "hierarchy4_id      0\n",
      "hierarchy5_id      0\n",
      "dtype: int64\n",
      "Accuracy: 0.6076923076923076\n",
      "Precision: 0.07022222222222223\n",
      "Recall: 0.10449735449735449\n",
      "F1-score: 0.08399787347155768\n",
      "    product_id  product_length  product_depth  product_width cluster_id  \\\n",
      "1        P0001            13.5           22.0           20.0  cluster_5   \n",
      "2        P0002            22.0           40.0           22.0  cluster_0   \n",
      "3        P0004             2.0           13.0            4.0  cluster_3   \n",
      "4        P0005            16.0           30.0           16.0  cluster_9   \n",
      "5        P0006             8.5           15.0           15.0  cluster_0   \n",
      "..         ...             ...            ...            ...        ...   \n",
      "595      P0636             5.0            5.0           21.5  cluster_0   \n",
      "672      P0719             6.0           17.0            6.0  cluster_4   \n",
      "675      P0722             5.0            7.0            5.0  cluster_0   \n",
      "679      P0727             2.0            9.5            7.0  cluster_0   \n",
      "694      P0744             2.0            7.0            7.0  cluster_0   \n",
      "\n",
      "    hierarchy1_id hierarchy2_id hierarchy3_id hierarchy4_id hierarchy5_id  \n",
      "1             H01         H0105       H010501     H01050100   H0105010006  \n",
      "2             H03         H0315       H031508     H03150800   H0315080028  \n",
      "3             H03         H0314       H031405     H03140500   H0314050003  \n",
      "4             H03         H0312       H031211     H03121109   H0312110917  \n",
      "5             H03         H0316       H031608     H03160817   H0316081708  \n",
      "..            ...           ...           ...           ...           ...  \n",
      "595           H01         H0106       H010601     H01060113   H0106011305  \n",
      "672           H00         H0000       H000004     H00000400   H0000040001  \n",
      "675           H01         H0107       H010702     H01070210   H0107021003  \n",
      "679           H02         H0209       H020900     H02090000   H0209000001  \n",
      "694           H03         H0311       H031108     H03110802   H0311080202  \n",
      "\n",
      "[699 rows x 10 columns]\n",
      "product_id        0\n",
      "product_length    0\n",
      "product_depth     0\n",
      "product_width     0\n",
      "cluster_id        0\n",
      "hierarchy1_id     0\n",
      "hierarchy2_id     0\n",
      "hierarchy3_id     0\n",
      "hierarchy4_id     0\n",
      "hierarchy5_id     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "product_df = pd.read_csv(\"product.csv\")\n",
    "\n",
    "# Check for nulls and replace missing values\n",
    "print(product_df.isna().sum())\n",
    "product_df[\"product_length\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "product_df[\"product_depth\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "product_df[\"product_width\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "print(product_df.isna().sum())\n",
    "\n",
    "# Split the data into complete and missing\n",
    "complete_data = product_df.dropna()\n",
    "missing_data = product_df[product_df['cluster_id'].isnull()].drop('cluster_id', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    complete_data[['product_length', 'product_depth', 'product_width']],\n",
    "    complete_data['cluster_id'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform K-nearest neighbors classification with hyperparameter tuning\n",
    "parameters = {\n",
    "    'n_neighbors': range(1, 21),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_weights = grid_search.best_params_['weights']\n",
    "best_algorithm = grid_search.best_params_['algorithm']\n",
    "best_leaf_size = grid_search.best_params_['leaf_size']\n",
    "best_p = grid_search.best_params_['p']\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Predict clusters for missing data using the best model\n",
    "missing_data['cluster_id'] = best_model.predict(missing_data[['product_length', 'product_depth', 'product_width']])\n",
    "\n",
    "# Concatenate the complete data with the imputed missing data\n",
    "result_df = pd.concat([complete_data, missing_data])\n",
    "\n",
    "# Print the resulting DataFrame with clusters\n",
    "print(result_df)\n",
    "print(result_df.isna().sum())\n",
    "\n",
    "result_df.to_csv(\"product_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_id                       0\n",
      "product_id                     0\n",
      "date                           0\n",
      "sales                          0\n",
      "revenue                        0\n",
      "stock                          0\n",
      "price                      89409\n",
      "promo_type_1                   0\n",
      "promo_bin_1              7401743\n",
      "promo_type_2                   0\n",
      "promo_bin_2              8575279\n",
      "promo_discount_2         8575279\n",
      "promo_discount_type_2    8575279\n",
      "dtype: int64\n",
      "store_id                       0\n",
      "product_id                     0\n",
      "date                           0\n",
      "sales                          0\n",
      "revenue                        0\n",
      "stock                          0\n",
      "price                       6670\n",
      "promo_type_1                   0\n",
      "promo_bin_1              7401743\n",
      "promo_type_2                   0\n",
      "promo_bin_2              8575279\n",
      "promo_discount_2         8575279\n",
      "promo_discount_type_2    8575279\n",
      "dtype: int64\n",
      "promo_type_2\n",
      "PR03    8575279\n",
      "PR02       6120\n",
      "PR01       2275\n",
      "PR04         88\n",
      "Name: count, dtype: int64\n",
      "promo_type_1\n",
      "PR14    7401743\n",
      "PR05     522735\n",
      "PR10     206649\n",
      "PR03     149192\n",
      "PR06     116494\n",
      "PR07      55586\n",
      "PR12      38979\n",
      "PR09      32233\n",
      "PR17      32006\n",
      "PR01      12163\n",
      "PR11       5007\n",
      "PR08       4176\n",
      "PR04       3383\n",
      "PR18       1878\n",
      "PR16        912\n",
      "PR13        607\n",
      "PR15         19\n",
      "Name: count, dtype: int64\n",
      "promo_bin_1\n",
      "verylow     497813\n",
      "low         247043\n",
      "moderate    184005\n",
      "high        137541\n",
      "veryhigh    115617\n",
      "Name: count, dtype: int64\n",
      "store_id        0\n",
      "product_id      0\n",
      "date            0\n",
      "sales           0\n",
      "revenue         0\n",
      "stock           0\n",
      "price           0\n",
      "promo_type_1    0\n",
      "promo_bin_1     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sales_df = pd.read_csv(\"sales.csv\")\n",
    "sales_df = sales_df[sales_df[\"date\"] < \"2019-10-01\"]\n",
    "sales_df = sales_df.drop(sales_df.columns[0], axis=1)\n",
    "print(sales_df.isna().sum())\n",
    "\n",
    "#fill the price\n",
    "sales_df['price'] = sales_df.groupby('product_id')['price'].transform(lambda x: x.fillna(x.mean()))\n",
    "print(sales_df.isna().sum())\n",
    "#after this some nulls still remain, that means that the dataset contains products for which there isnt a price in any place\n",
    "\n",
    "#drop columns with a lot of nulls\n",
    "sales_df.drop(['promo_bin_2', 'promo_discount_2', 'promo_discount_type_2'], axis=1, inplace=True)\n",
    "\n",
    "#count distinct values of a column\n",
    "value_counts = sales_df['promo_type_2'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "# Given that less than one percent of the data related to promo_type_2 is different from the majority of the values\n",
    "# we will drop this column aswell\n",
    "sales_df.drop(['promo_type_2'], axis=1, inplace=True)\n",
    "\n",
    "value_counts = sales_df['promo_type_1'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "value_counts = sales_df['promo_bin_1'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "sales_df['promo_bin_1'].fillna(\"none\", inplace=True)\n",
    "\n",
    "#binary_cols = pd.get_dummies(sales_df['promo_bin_1'], prefix='promo_bin_')\n",
    "#sales_df = pd.concat([sales_df, binary_cols], axis=1)\n",
    "\n",
    "#binary_cols = pd.get_dummies(sales_df['promo_type_1'], prefix='promo_type_')\n",
    "#sales_df = pd.concat([sales_df, binary_cols], axis=1)\n",
    "\n",
    "#sales_df.drop(['promo_type_1', 'promo_bin_1'], axis=1, inplace=True)\n",
    "\n",
    "# Dropping the remaing null values of the price its around 7000 so less then 1% see after model if it affects a lot\n",
    "sales_df.dropna(inplace=True)\n",
    "print(sales_df.isna().sum())\n",
    "sales_df.head()\n",
    "sales_df.to_csv(\"sales_df.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding external variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store_id storetype_id  store_size city_id_old country_id      city_code\n",
      "0     S0036         ST04          21        C001     TURKEY        Denizli\n",
      "1     S0005         ST04          19        C001     TURKEY        Denizli\n",
      "2     S0104         ST04          47        C002     TURKEY         Ankara\n",
      "3     S0068         ST03          14        C003     TURKEY          Izmir\n",
      "4     S0086         ST03          12        C003     TURKEY          Izmir\n",
      "..      ...          ...         ...         ...        ...            ...\n",
      "58    S0040         ST04          42        C017     TURKEY  Kahramanmaraş\n",
      "59    S0046         ST04          26        C017     TURKEY  Kahramanmaraş\n",
      "60    S0102         ST04          27        C018     TURKEY        Erzurum\n",
      "61    S0032         ST03          14        C019     TURKEY            Van\n",
      "62    S0061         ST04          35        C019     TURKEY            Van\n",
      "\n",
      "[63 rows x 6 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Population'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Population'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(cities_df)\n\u001b[0;32m      8\u001b[0m \u001b[39m#It doesnt work for Izmir so replace manually\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m cities_df[\u001b[39m'\u001b[39m\u001b[39mPopulation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cities_df[\u001b[39m'\u001b[39;49m\u001b[39mPopulation\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mPopulation not found\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2,847,691\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39m# remove the commas\u001b[39;00m\n\u001b[0;32m     12\u001b[0m cities_df[\u001b[39m'\u001b[39m\u001b[39mPopulation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cities_df[\u001b[39m'\u001b[39m\u001b[39mPopulation\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Population'"
     ]
    }
   ],
   "source": [
    "# Add a new column to store population\n",
    "#cities_df['Population'] = cities_df['city_code'].apply(get_city_population)\n",
    "#cities_df['Latitude'] = cities_df['city_code'].apply(latitude)\n",
    "#cities_df['Longitude'] = cities_df['city_code'].apply(longitude)\n",
    "\n",
    "print(cities_df)\n",
    "\n",
    "#It doesnt work for Izmir so replace manually\n",
    "cities_df['Population'] = cities_df['Population'].str.replace('Population not found', '2,847,691')\n",
    "\n",
    "# remove the commas\n",
    "cities_df['Population'] = cities_df['Population'].str.replace(',', '')\n",
    "\n",
    "cities_df.to_csv(\"cities_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date\n",
      "0      2017-01-02\n",
      "183    2017-01-03\n",
      "364    2017-01-04\n",
      "544    2017-01-05\n",
      "724    2017-01-06\n",
      "...           ...\n",
      "223490 2019-09-26\n",
      "223739 2019-09-27\n",
      "223986 2019-09-28\n",
      "224235 2019-09-29\n",
      "224483 2019-09-30\n",
      "\n",
      "[1002 rows x 1 columns]\n",
      "             date  season\n",
      "0      2017-01-02  winter\n",
      "183    2017-01-03  winter\n",
      "364    2017-01-04  winter\n",
      "544    2017-01-05  winter\n",
      "724    2017-01-06  winter\n",
      "...           ...     ...\n",
      "223490 2019-09-26  autumn\n",
      "223739 2019-09-27  autumn\n",
      "223986 2019-09-28  autumn\n",
      "224235 2019-09-29  autumn\n",
      "224483 2019-09-30  autumn\n",
      "\n",
      "[1002 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"sales_df.csv\")\n",
    "df_1 = df_1[['date']].copy()\n",
    "df_1.drop_duplicates(inplace=True)\n",
    "df_1['date'] = pd.to_datetime(df_1['date'])\n",
    "print(df_1)\n",
    "df_1['season'] = df_1['date'].apply(season)\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date  season\n",
      "0      2017-01-02  winter\n",
      "183    2017-01-03  winter\n",
      "364    2017-01-04  winter\n",
      "544    2017-01-05  winter\n",
      "724    2017-01-06  winter\n",
      "...           ...     ...\n",
      "223490 2019-09-26  autumn\n",
      "223739 2019-09-27  autumn\n",
      "223986 2019-09-28  autumn\n",
      "224235 2019-09-29  autumn\n",
      "224483 2019-09-30  autumn\n",
      "\n",
      "[1002 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#df_1['holidays'] = df_1['date'].apply(holidays)\n",
    "print(df_1)\n",
    "\n",
    "#change some values manually\n",
    "df_1.to_csv(\"season_holidays.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming data for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      store_id        date storetype_id  store_size city_id_old country_id  \\\n",
      "0        S0002  2017-01-02         ST04          39        C007     TURKEY   \n",
      "1        S0002  2017-01-03         ST04          39        C007     TURKEY   \n",
      "2        S0002  2017-01-04         ST04          39        C007     TURKEY   \n",
      "3        S0002  2017-01-05         ST04          39        C007     TURKEY   \n",
      "4        S0002  2017-01-06         ST04          39        C007     TURKEY   \n",
      "...        ...         ...          ...         ...         ...        ...   \n",
      "57646    S0143  2019-09-26         ST03          13        C014     TURKEY   \n",
      "57647    S0143  2019-09-27         ST03          13        C014     TURKEY   \n",
      "57648    S0143  2019-09-28         ST03          13        C014     TURKEY   \n",
      "57649    S0143  2019-09-29         ST03          13        C014     TURKEY   \n",
      "57650    S0143  2019-09-30         ST03          13        C014     TURKEY   \n",
      "\n",
      "      city_code  \n",
      "0         Adana  \n",
      "1         Adana  \n",
      "2         Adana  \n",
      "3         Adana  \n",
      "4         Adana  \n",
      "...         ...  \n",
      "57646  Istanbul  \n",
      "57647  Istanbul  \n",
      "57648  Istanbul  \n",
      "57649  Istanbul  \n",
      "57650  Istanbul  \n",
      "\n",
      "[57651 rows x 7 columns]\n",
      "        city_code        date\n",
      "0           Adana  2017-01-02\n",
      "1           Adana  2017-01-03\n",
      "2           Adana  2017-01-04\n",
      "3           Adana  2017-01-05\n",
      "4           Adana  2017-01-06\n",
      "...           ...         ...\n",
      "54299  Diyarbakır  2019-09-26\n",
      "54300  Diyarbakır  2019-09-27\n",
      "54301  Diyarbakır  2019-09-28\n",
      "54302  Diyarbakır  2019-09-29\n",
      "54303  Diyarbakır  2019-09-30\n",
      "\n",
      "[18643 rows x 2 columns]\n",
      "18643\n"
     ]
    }
   ],
   "source": [
    "unique_combinations = sales_df[['store_id', 'date']].drop_duplicates()\n",
    "merged_df = unique_combinations.merge(cities_df, on='store_id', how='left')\n",
    "print(merged_df)\n",
    "unique_combinations_2 = merged_df[['city_code', 'date']].drop_duplicates()\n",
    "print(unique_combinations_2)\n",
    "print(len(unique_combinations_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     total_sales  total_revenue  total_stock  average_price  \\\n",
      "date       store_id                                                           \n",
      "2017-01-02 S0002            74.0          172.0       2633.0           11.0   \n",
      "           S0003            32.0           67.0       1075.0            6.0   \n",
      "           S0010           103.0          365.0       1449.0            8.0   \n",
      "           S0012            38.0           69.0       1936.0            9.0   \n",
      "           S0014            58.0          217.0        774.0            6.0   \n",
      "...                          ...            ...          ...            ...   \n",
      "2019-09-30 S0131           399.0         2019.0      12128.0           18.0   \n",
      "           S0132           511.0         3381.0      14995.0           28.0   \n",
      "           S0141           100.0          309.0       4481.0           15.0   \n",
      "           S0142           312.0         2020.0      23189.0           20.0   \n",
      "           S0143           152.0          756.0       4406.0           14.0   \n",
      "\n",
      "                     promo_type_count  promo_bin_count  \n",
      "date       store_id                                     \n",
      "2017-01-02 S0002                    6                5  \n",
      "           S0003                    6                4  \n",
      "           S0010                    6                5  \n",
      "           S0012                    6                5  \n",
      "           S0014                    6                4  \n",
      "...                               ...              ...  \n",
      "2019-09-30 S0131                    9                6  \n",
      "           S0132                   11                6  \n",
      "           S0141                    8                5  \n",
      "           S0142                   10                6  \n",
      "           S0143                    7                5  \n",
      "\n",
      "[8287 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      total_sales  total_revenue  total_stock  average_price  \\\n",
       "date       store_id                                                           \n",
       "2017-01-02 S0002            74.0          172.0       2633.0           11.0   \n",
       "           S0003            32.0           67.0       1075.0            6.0   \n",
       "           S0010           103.0          365.0       1449.0            8.0   \n",
       "           S0012            38.0           69.0       1936.0            9.0   \n",
       "           S0014            58.0          217.0        774.0            6.0   \n",
       "...                          ...            ...          ...            ...   \n",
       "2019-09-30 S0131           399.0         2019.0      12128.0           18.0   \n",
       "           S0132           511.0         3381.0      14995.0           28.0   \n",
       "           S0141           100.0          309.0       4481.0           15.0   \n",
       "           S0142           312.0         2020.0      23189.0           20.0   \n",
       "           S0143           152.0          756.0       4406.0           14.0   \n",
       "\n",
       "                     promo_type_count  promo_bin_count  \n",
       "date       store_id                                     \n",
       "2017-01-02 S0002                    6                5  \n",
       "           S0003                    6                4  \n",
       "           S0010                    6                5  \n",
       "           S0012                    6                5  \n",
       "           S0014                    6                4  \n",
       "...                               ...              ...  \n",
       "2019-09-30 S0131                    9                6  \n",
       "           S0132                   11                6  \n",
       "           S0141                    8                5  \n",
       "           S0142                   10                6  \n",
       "           S0143                    7                5  \n",
       "\n",
       "[8287 rows x 6 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df['date'] = pd.to_datetime(sales_df['date'])  # Convert 'date' column to datetime type\n",
    "\n",
    "future_ts = sales_df.groupby([pd.Grouper(key='date', freq='W-MON'), 'store_id']) \\\n",
    "    .agg(total_sales=('sales', 'sum'),\n",
    "         total_revenue=('revenue', 'sum'),\n",
    "         total_stock=('stock', 'sum'),\n",
    "         average_price=('price', 'mean'),\n",
    "         promo_type_count=('promo_type_1', 'nunique'),\n",
    "         promo_bin_count=('promo_bin_1', 'nunique')) \\\n",
    "    .round()\n",
    "\n",
    "print(future_ts)\n",
    "future_ts.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilizador\\AppData\\Local\\Temp\\ipykernel_6508\\3213944710.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  one_store.drop(['store_id'], axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            total_sales  total_revenue  average_price  promo_type_count  \\\n",
      "date                                                                      \n",
      "2017-01-02         74.0          172.0           11.0                 6   \n",
      "2017-01-09        820.0         1974.0           11.0                 8   \n",
      "2017-01-16        851.0         1834.0           11.0                 5   \n",
      "2017-01-23       1031.0         7377.0           11.0                10   \n",
      "2017-01-30        910.0         2037.0           11.0                 8   \n",
      "...                 ...            ...            ...               ...   \n",
      "2019-09-02        599.0         4083.0           31.0                10   \n",
      "2019-09-09        623.0         6283.0           31.0                 8   \n",
      "2019-09-16        655.0         4993.0           25.0                12   \n",
      "2019-09-23        504.0         3064.0           26.0                 9   \n",
      "2019-09-30        646.0         5662.0           29.0                11   \n",
      "\n",
      "            promo_bin_count  \n",
      "date                         \n",
      "2017-01-02                5  \n",
      "2017-01-09                6  \n",
      "2017-01-16                6  \n",
      "2017-01-23                6  \n",
      "2017-01-30                5  \n",
      "...                     ...  \n",
      "2019-09-02                6  \n",
      "2019-09-09                6  \n",
      "2019-09-16                6  \n",
      "2019-09-23                6  \n",
      "2019-09-30                6  \n",
      "\n",
      "[144 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilizador\\AppData\\Local\\Temp\\ipykernel_6508\\3213944710.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['predictions'] = predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  total_sales  predictions\n",
      "142 2019-09-23        504.0   700.534998\n",
      "143 2019-09-30        646.0   716.618407\n",
      "MAE: 133.57670266274908\n",
      "MSE: 21806.482475649616\n",
      "RMSE: 147.67018140318518\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'holidays'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'holidays'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m sales_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39msales_df.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m holidays_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mseason_holidays.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m holidays_df\u001b[39m.\u001b[39mloc[holidays_df[\u001b[39m'\u001b[39;49m\u001b[39mholidays\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNot Holiday\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mholidays\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHoliday\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     55\u001b[0m holidays_df[\u001b[39m'\u001b[39m\u001b[39mholidays\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m holidays_df[\u001b[39m'\u001b[39m\u001b[39mholidays\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHoliday\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m     56\u001b[0m holidays_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mget_dummies(holidays_df, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mseason\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Utilizador\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'holidays'"
     ]
    }
   ],
   "source": [
    "#TESTING AREA\n",
    "sales_df = pd.read_csv(\"sales_df.csv\")\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "one_store = sales_df.query(\"store_id == 'S0002'\")\n",
    "one_store.drop(['store_id'], axis=1, inplace=True)\n",
    "future_ts = one_store.groupby([pd.Grouper(key='date', freq='W-MON')]) \\\n",
    "    .agg(total_sales=('sales', 'sum'),\n",
    "         total_revenue=('revenue', 'sum'),\n",
    "         average_price=('price', 'mean'),\n",
    "         promo_type_count=('promo_type_1', 'nunique'),\n",
    "         promo_bin_count=('promo_bin_1', 'nunique')) \\\n",
    "    .round()\n",
    "\n",
    "print(future_ts)\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# explorar dataframe\n",
    "future_ts.head()\n",
    "future_ts.info()\n",
    "future_ts.plot()\n",
    "\n",
    "# decomposicao ETS (Error, Trend, Seasonality)\n",
    "result = seasonal_decompose(future_ts['total_sales'], model ='multiplicative')\n",
    "\n",
    "future_ts = future_ts.reset_index()\n",
    "# Split the data into training and test sets\n",
    "train_data = future_ts[:-2]  # Use all data except the last two weeks for training\n",
    "test_data = future_ts[-2:]  # Last two weeks for testing\n",
    "\n",
    "# Fit the auto_arima model to the training data\n",
    "model = auto_arima(train_data['total_sales'], seasonal=True, m=52)\n",
    "\n",
    "# Generate predictions for the test data (last two weeks)\n",
    "predictions = model.predict(n_periods=len(test_data))\n",
    "\n",
    "# Combine the test data with the predictions\n",
    "test_data['predictions'] = predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(test_data['total_sales'], test_data['predictions'])\n",
    "mse = mean_squared_error(test_data['total_sales'], test_data['predictions'])\n",
    "rmse = mean_squared_error(test_data['total_sales'], test_data['predictions'], squared=False)\n",
    "\n",
    "# Print the test data, predictions, and evaluation metrics\n",
    "print(test_data[['date', 'total_sales', 'predictions']])\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "sales_df = pd.read_csv(\"sales_df.csv\")\n",
    "holidays_df = pd.read_csv(\"season_holidays.csv\")\n",
    "\n",
    "holidays_df.loc[holidays_df['holidays'] != 'Not Holiday', 'holidays'] = 'Holiday'\n",
    "holidays_df['holidays'] = holidays_df['holidays'].apply(lambda x: 1 if x == 'Holiday' else 0)\n",
    "holidays_df = pd.get_dummies(holidays_df, columns=['season'])\n",
    "\n",
    "one_store = sales_df.query(\"store_id == 'S0002'\")\n",
    "\n",
    "one_store = one_store.groupby([pd.Grouper(key='date')]) \\\n",
    "    .agg(total_sales=('sales', 'sum'),\n",
    "         total_revenue=('revenue', 'sum'),\n",
    "         average_price=('price', 'mean'))\\\n",
    "    .round()\n",
    "\n",
    "merged_df = one_store.merge(holidays_df, on='date', how='left')\n",
    "\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "print(merged_df)\n",
    "\"\"\"\n",
    "Careful to run it hasnt been tuned, it takes a long time for now\n",
    "\"\"\"\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# explorar dataframe\n",
    "merged_df.head()\n",
    "merged_df.info()\n",
    "merged_df.plot()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = merged_df[:-7]  # Use all data except the last two weeks for training\n",
    "test_data = merged_df[-7:]  # Last two weeks for testing\n",
    "\n",
    "# Fit the auto_arima model to the training data\n",
    "model = auto_arima(train_data['total_sales'], seasonal=True, m=365)\n",
    "\n",
    "# Generate predictions for the test data (last two weeks)\n",
    "predictions = model.predict(n_periods=len(test_data))\n",
    "\n",
    "# Combine the test data with the predictions\n",
    "test_data['predictions'] = predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(test_data['total_sales'], test_data['predictions'])\n",
    "mse = mean_squared_error(test_data['total_sales'], test_data['predictions'])\n",
    "rmse = mean_squared_error(test_data['total_sales'], test_data['predictions'], squared=False)\n",
    "\n",
    "# Print the test data, predictions, and evaluation metrics\n",
    "print(test_data[['date', 'total_sales', 'predictions']])\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(test_data.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilizador\\AppData\\Local\\Temp\\ipykernel_6508\\4195866984.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  one_store.drop(['store_id'], axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 69.76192852212989\n",
      "Melhores parâmetros: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "RMSE: 120.61425511926775\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Carregar os dados\n",
    "sales_df = pd.read_csv(\"sales_df.csv\")\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "one_store = sales_df.query(\"store_id == 'S0002'\")\n",
    "one_store.drop(['store_id'], axis=1, inplace=True)\n",
    "future_ts = one_store.groupby([pd.Grouper(key='date', freq='W-MON')]) \\\n",
    "    .agg(total_sales=('sales', 'sum'),\n",
    "         total_revenue=('revenue', 'sum'),\n",
    "         average_price=('price', 'mean'),\n",
    "         promo_type_count=('promo_type_1', 'nunique'),\n",
    "         promo_bin_count=('promo_bin_1', 'nunique')) \\\n",
    "    .round()\n",
    "\n",
    "train_data = future_ts[:-2]  \n",
    "test_data = future_ts[-2:] \n",
    "\n",
    "X_train = train_data[['total_revenue', 'average_price', 'promo_type_count','promo_bin_count']]  \n",
    "y_train = train_data['total_sales'] \n",
    "\n",
    "X_test = train_data[['total_revenue', 'average_price', 'promo_type_count','promo_bin_count']]  \n",
    "y_test = train_data['total_sales']  \n",
    "\n",
    "\n",
    "#Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação do modelo\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Tunning do modelo\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Criar o modelo de ensemble (Random Forest)\n",
    "model_bp = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Realizar busca em grade para encontrar os melhores parâmetros\n",
    "grid_search = GridSearchCV(estimator=model_bp, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print('Melhores parâmetros:', best_params)\n",
    "\n",
    "# Avaliação do modelo com os melhores parâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "print(f'RMSE: {rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
