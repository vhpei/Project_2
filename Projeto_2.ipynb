{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of python libraries\n",
    "\n",
    "In this section there are all the libraries used in the entire file listed so that it may be easy to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from math import sqrt\n",
    "from geopy.geocoders import Nominatim\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used along the code\n",
    "\n",
    "All the function used along the code are listed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_population(city):\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_largest_cities_and_towns_in_Turkey'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='wikitable')\n",
    "    rows = table.find_all('tr')[1:]  # Exclude the header row\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if columns[1].text.strip() == city:\n",
    "            population = columns[6].text.strip()\n",
    "            return population\n",
    "    return 'Population not found'\n",
    "\n",
    "def latitude(city):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.geocode(city)\n",
    "    if location is not None:\n",
    "        latitude = location.latitude\n",
    "        return latitude\n",
    "    else:\n",
    "        return \"Latitude not found\"\n",
    "    \n",
    "def longitude(city):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.geocode(city)\n",
    "    if location is not None:\n",
    "        longitude = location.longitude\n",
    "        return longitude\n",
    "    else:\n",
    "        return \"Longitude not found\"\n",
    "    \n",
    "def season(date):\n",
    "    month = date.strftime('%B')\n",
    "    day = date.day\n",
    "    if month in ('January', 'February', 'March'):\n",
    "        season = 'winter'\n",
    "    elif month in ('April', 'May', 'June'):\n",
    "        season = 'spring'\n",
    "    elif month in ('July', 'August', 'September'):\n",
    "        season = 'summer'\n",
    "    else:\n",
    "        season = 'autumn'\n",
    "    if (month == 'March') and (day > 20):\n",
    "        season = 'spring'\n",
    "    elif (month == 'June') and (day > 20):\n",
    "        season = 'summer'\n",
    "    elif (month == 'September') and (day > 22):\n",
    "        season = 'autumn'\n",
    "    elif (month == 'December') and (day > 21):\n",
    "        season = 'winter'\n",
    "    return season\n",
    "\n",
    "def holidays(date):\n",
    "    year = date.strftime('%Y')\n",
    "    formated = date.strftime('%#d %B')\n",
    "    url = 'https://en.wikipedia.org/wiki/Public_holidays_in_Turkey'\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the holiday data\n",
    "    table = soup.find_all('table', 'wikitable')[0]\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        # Extract the cells of each row\n",
    "        columns = row.find_all('td')\n",
    "        if columns[0].text.strip() == formated:\n",
    "            name = columns[1].text.strip()\n",
    "            return name\n",
    "    return 'Not Holiday'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "In this section some data cleaning will be performed. The city names were corrected, null values were replaced using the interpolate function of pandas, aswell as a k-nearest neighbors algorithm, for the first 2 csv files.  \n",
    "For the third csv file, some columns were droped, and null values were filed, around 7000 null values remain which means that ir represents less than 1 % of the entire csv file, so we made the decision to drop those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.read_csv(\"cities.csv\")\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('?', 'i')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Sanliurfa', 'Şanlıurfa')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Izmir', 'İzmir')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Diyarbakir', 'Diyarbakır')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Eskiiehir', 'Eskişehir')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Adapazari', 'Adapazarı')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.replace('Kahramanmaras', 'Kahramanmaraş')\n",
    "cities_df['city_code'] = cities_df['city_code'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv(\"product.csv\")\n",
    "\n",
    "# Check for nulls and replace missing values\n",
    "print(product_df.isna().sum())\n",
    "product_df[\"product_length\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "product_df[\"product_depth\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "product_df[\"product_width\"].interpolate(method ='linear', limit_direction ='both', inplace=True)\n",
    "print(product_df.isna().sum())\n",
    "\n",
    "# Split the data into complete and missing\n",
    "complete_data = product_df.dropna()\n",
    "missing_data = product_df[product_df['cluster_id'].isnull()].drop('cluster_id', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    complete_data[['product_length', 'product_depth', 'product_width']],\n",
    "    complete_data['cluster_id'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform K-nearest neighbors classification with hyperparameter tuning\n",
    "parameters = {\n",
    "    'n_neighbors': range(1, 21),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform K-nearest neighbors classification with hyperparameter tuning\n",
    "parameters = {\n",
    "    'n_neighbors': range(1, 21),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_weights = grid_search.best_params_['weights']\n",
    "best_algorithm = grid_search.best_params_['algorithm']\n",
    "best_leaf_size = grid_search.best_params_['leaf_size']\n",
    "best_p = grid_search.best_params_['p']\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Predict clusters for missing data using the best model\n",
    "missing_data['cluster_id'] = best_model.predict(missing_data[['product_length', 'product_depth', 'product_width']])\n",
    "\n",
    "# Concatenate the complete data with the imputed missing data\n",
    "result_df = pd.concat([complete_data, missing_data])\n",
    "\n",
    "# Print the resulting DataFrame with clusters\n",
    "print(result_df)\n",
    "print(result_df.isna().sum())\n",
    "\n",
    "result_df.to_csv(\"product_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(\"sales.csv\")\n",
    "sales_df = sales_df[sales_df[\"date\"] < \"2019-10-01\"]\n",
    "sales_df = sales_df.drop(sales_df.columns[0], axis=1)\n",
    "print(sales_df.isna().sum())\n",
    "\n",
    "#fill the price\n",
    "sales_df['price'] = sales_df.groupby('product_id')['price'].transform(lambda x: x.fillna(x.mean()))\n",
    "print(sales_df.isna().sum())\n",
    "#after this some nulls still remain, that means that the dataset contains products for which there isnt a price in any place\n",
    "\n",
    "#drop columns with a lot of nulls\n",
    "sales_df.drop(['promo_bin_2', 'promo_discount_2', 'promo_discount_type_2'], axis=1, inplace=True)\n",
    "\n",
    "#count distinct values of a column\n",
    "value_counts = sales_df['promo_type_2'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "# Given that less than one percent of the data related to promo_type_2 is different from the majority of the values\n",
    "# we will drop this column aswell\n",
    "sales_df.drop(['promo_type_2'], axis=1, inplace=True)\n",
    "\n",
    "value_counts = sales_df['promo_type_1'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "value_counts = sales_df['promo_bin_1'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "sales_df['promo_bin_1'].fillna(\"none\", inplace=True)\n",
    "\n",
    "#binary_cols = pd.get_dummies(sales_df['promo_bin_1'], prefix='promo_bin_')\n",
    "#sales_df = pd.concat([sales_df, binary_cols], axis=1)\n",
    "\n",
    "#binary_cols = pd.get_dummies(sales_df['promo_type_1'], prefix='promo_type_')\n",
    "#sales_df = pd.concat([sales_df, binary_cols], axis=1)\n",
    "\n",
    "#sales_df.drop(['promo_type_1', 'promo_bin_1'], axis=1, inplace=True)\n",
    "\n",
    "# Dropping the remaing null values of the price its around 7000 so less then 1% see after model if it affects a lot\n",
    "sales_df.dropna(inplace=True)\n",
    "print(sales_df.isna().sum())\n",
    "sales_df.head()\n",
    "sales_df.to_csv(\"sales_df.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding external variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to store population\n",
    "#cities_df['Population'] = cities_df['city_code'].apply(get_city_population)\n",
    "#cities_df['Latitude'] = cities_df['city_code'].apply(latitude)\n",
    "#cities_df['Longitude'] = cities_df['city_code'].apply(longitude)\n",
    "\n",
    "print(cities_df)\n",
    "\n",
    "#It doesnt work for Izmir so replace manually\n",
    "cities_df['Population'] = cities_df['Population'].str.replace('Population not found', '2,847,691')\n",
    "\n",
    "# remove the commas\n",
    "cities_df['Population'] = cities_df['Population'].str.replace(',', '')\n",
    "\n",
    "cities_df.to_csv(\"cities_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"sales_df.csv\")\n",
    "df_1 = df_1[['date']].copy()\n",
    "df_1.drop_duplicates(inplace=True)\n",
    "df_1['date'] = pd.to_datetime(df_1['date'])\n",
    "print(df_1)\n",
    "df_1['season'] = df_1['date'].apply(season)\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1['holidays'] = df_1['date'].apply(holidays)\n",
    "print(df_1)\n",
    "\n",
    "#change some values manually\n",
    "df_1.to_csv(\"season_holidays.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming data for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combinations = sales_df[['store_id', 'date']].drop_duplicates()\n",
    "merged_df = unique_combinations.merge(cities_df, on='store_id', how='left')\n",
    "print(merged_df)\n",
    "unique_combinations_2 = merged_df[['city_code', 'date']].drop_duplicates()\n",
    "print(unique_combinations_2)\n",
    "print(len(unique_combinations_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['date'] = pd.to_datetime(sales_df['date'])  # Convert 'date' column to datetime type\n",
    "\n",
    "future_ts = sales_df.groupby([pd.Grouper(key='date', freq='W-MON'), 'store_id']) \\\n",
    "    .agg(total_sales=('sales', 'sum'),\n",
    "         total_revenue=('revenue', 'sum'),\n",
    "         total_stock=('stock', 'sum'),\n",
    "         average_price=('price', 'mean'),\n",
    "         promo_type_count=('promo_type_1', 'nunique'),\n",
    "         promo_bin_count=('promo_bin_1', 'nunique')) \\\n",
    "    .round()\n",
    "\n",
    "print(future_ts)\n",
    "future_ts.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
